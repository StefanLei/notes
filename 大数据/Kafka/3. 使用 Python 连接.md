#### Python 连接 Kafka

我们使用的是 pykafka 库

> 消费者

```python
from pykafka import KafkaClient

# 既可以使用 zookeeper 连接，又可以使用 kafka 连接
zookeepers = "hadoop1:2181"
client = KafkaClient(zookeeper_hosts=zookeepers)

# hosts = "hadoop1:9092,hadoop2:9092,hadoop3:9092,hadoop4:9092"
# client = KafkaClient(hosts=hosts)


# auto_commit_enable=True,auto_commit_interval_ms=1 
# 这两个参数的作用是，是自动把已经获取的消息的offset提交给 zookeeper ，保证一个 group 只读取一边消息。
# 两个参数要同时使用

# consumer_group 是必须的
# consumer_id 是可选的

topic = client.topics['test'.endoce()]
consumer = topic.get_simple_consumer(consumer_group=b'one', consumer_id=b'one', auto_commit_enable=True,auto_commit_interval_ms=1)

for message in consumer:
    offset = message.offset
    value = message.value
    print(offset, value.decode('utf8'))
```



##### 保证每个 group 消费一次，继续上次的消费的位置。每个 group 相互独立

```python
consumer = topic.get_simple_consumer(consumer_group=b'one', consumer_id=b'one', auto_commit_enable=True,auto_commit_interval_ms=1)
```

##### 从头开始消费所有的（无限次）(一般用于测试，实际业务需要指定 auto_commit)

```python
consumer = topic.get_simple_consumer(consumer_group=b'one', consumer_id=b'one')
```

---

> 生产者

```python
from pykafka import KafkaClient
import time

client = KafkaClient(zookeeper_hosts="hadoop1:2181")

topic = client.topics['test'.encode()]

# 获取一个生产者对象，sync=True 意味着同步生产消息，就是说生产一个，消费者就可以消费一个。
# 如果为False那么就要，全部生产完了才可以开始消费
producer = topic.get_producer(sync=True)

for i in range(1000):
    print(i)
    producer.produce('message: {}'.format(i).encode('utf8'))
producer.stop()

```


### Spark RDD 编程模型

[参数链接](https://www.cnblogs.com/qingyunzong/p/8899715.html)

Spark 支持两个类型（算子）操作：`Transformation`和`Action`

---

#### `Transformation`

主要做的就是将一个已有的 RDD 生成另外一个 RDD。Transformation 具有 lazy 特性(延迟加载)。`Transformation`里面的代码不会真正的执行。只有当我们的程序里面遇到一个 `Action`算子的时候，代码才会真正的被执行。这种设定让 `Spark`更加有效的运行.

常用的 `Transformation`

```python
# 返回一个新的 RDD,该RDD由每一个插入的元素经过func函数转换而来
map(func)   

# 类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）
# 列表里面的每一个元素先由map处理，然后map处理后的中间产物，在由 flat 打散。
flatmap(func)

sortby(func)

sortByKey(func)
```



举例说明

==flatmap== 是先对第一个元素进行map操作，然后把map后的结果，接着进行 flat 操作，然后处理第二个元素。

```python
from pyspark import SparkConf, SparkContext

conf = SparkConf().setAppName('wordcount').setMaster('local')
sc = SparkContext(conf=conf)

data = [
    'I love you',
    'I love Python',
    'I love Java',
    'Python is the best',
]

# 序列化成 RDD 对象
data_rdd = sc.parallelize(data)

# 这里的结果是 ['I',' ','l','o','v','e',' ','y','o','u'........]
# 这里先由map处理，第一个元素被map处理后变成了 'I love you' (和原来一样)
# 然后再被 flat 处理（打散）['I',' ','l','o','v','e',' ','y','o','u']
# 然后data里面的每一个元素都是这样的过程，最后把所有的 flat 结果合并。就变成了上面的结果。
results = data_rdd.flatMap(lambda x: x).collect()

# data 里面第一个元素被 map 处理后，变成了 ['I','love','you'] （由字符串变成了列表）
# 然后 ['I','love','you'] 经过 flat 处理，就变成了打散的单词 'I','love','you'
# 然后其他的元素也是这样的过程，然后就把结果合并
results = data_rdd.flatMap(lambda x: x.split()).collect()

```

---

`Action`算子

`Action`动作算子是立刻执行的，执行的结果要么返回给用户，要么储存到文件里面

```python
# 将结果返回到 Driver 端
collect() 

# 可以迭代取出RDD上面的数据
foreach(func)

```





### Python 操作 Spark

==利用 pyspark 库== 

下面是在 Windows 上的单机测试模式

`pyspark`不支持 Python3.6 。下面的代码在 Python 3.5.5 下运行。如果 master 使用的是 Linux 上的集群，那么Linux上也要使用 Python 3.5.5 

```python
from pyspark import SparkConf, SparkContext

# 指定任务名字，以及使用哪台 master 服务器，
conf = SparkConf().setAppName('wordcount').setMaster('local')

conf = SparkConf().setAppName('wordcount').setMaster('spark://hadoop1:7077')
sc = SparkContext(conf=conf)

data = [
    'I love you',
    'I love Python',
    'I love Java',
    'Python is the best',
]

# 序列化成 RDD 对象
data_rdd = sc.parallelize(data)
results_split = data_rdd.flatMap(lambda x: x.split()).map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b)
print(results_split.collect())

```

案列2

```python
from pyspark import SparkConf, SparkContext
import time

conf = SparkConf().setAppName('wordcount').setMaster('local')
sc = SparkContext(conf=conf)

data = [
    'I love you',
    'I love Python',
    'I love Java'
]

data_rdd = sc.parallelize(data)

results1 = data_rdd.flatMap(lambda x: x.split()).map(lambda x: (x, 1)).reduceByKeyLocally(lambda a, b: a + b)
results2 = data_rdd.flatMap(lambda x: x.split()).map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b)

print(results1)
print(results2.collect())

# results1 结果以字典输出
{'love': 3, 'Java': 1, 'you': 1, 'Python': 1, 'I': 3}

# results2 结果以列表输出
[('Java', 1), ('you', 1), ('I', 3), ('Python', 1), ('love', 3)]

```


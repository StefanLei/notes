### Spark 安装模式

==Spark 包含多种安装模式==

我们用的 Spark 是 1.60，不支持 Python 3.6，我们用的是 Python 3.5

> 本地测试模式 local

在 Windows 平台上

- 安装好 Java 配置 JAVA_HOME  JAVA_HOME/bin
- 下载解压 Spark ，配置好 SPARK_HOME SPARK_HOME/bin
- 配置好 Python 3.5 环境

##### 使用方式一


然后直接在命令行中执行 pyspark

##### 使用方式二

使用 pyspark 这个第三方库

```python
from pyspark import SparkConf, SparkContext
import time

conf = SparkConf().setAppName('wordcount').setMaster('local')
sc = SparkContext(conf=conf)

data = [
    'I love you',
    'I love Python',
    'I love Java'
]

data_rdd = sc.parallelize(data)

results1 = data_rdd.flatMap(lambda x: x.split()).map(lambda x: (x, 1)).reduceByKeyLocally(lambda a, b: a + b)
results2 = data_rdd.flatMap(lambda x: x.split()).map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b)

print(results1)
print(results2.collect())

# results1 结果以字典输出
{'love': 3, 'Java': 1, 'you': 1, 'Python': 1, 'I': 3}

# results2 结果以列表输出
[('Java', 1), ('you', 1), ('I', 3), ('Python', 1), ('love', 3)]

```



> Standalone 集群环境

在 Linux 平台上

Hadoop1 : Master

Hadoop2 : Work

Hadoop3 : Work

- 下载 Spark 我们用的是 1.6 ，Python 3.5

- 解压

- 进入安装目录下的 conf 文件夹，把 slaves.template 改成 slaves ，同时在里面设置从节点

  ```shell
  mv slaves.template slaves
  vim slaves
  
  # 在里面设置从节点（work）
  hadoop2
  hadoop3
  ```

- 修改 conf 目录下的 spark-env.sh

  ```sh
  # 配置master节点地址
  export SPARK_MASTER_IP=hadoop1
  export SPARK_MASTER_PORT=7077
  export SPARK_WORKER_CORES=2
  
  export JAVA_HOME=
  # 这里是配置每个work的最大内存
  export SPARK_WORKER_MEMORY=3g
  # 指定是指定运行 python 任务的时候，使用哪个python版本
  # spark 默认是调用系统的 python 命令，系统默认是 python2 ，所以需要我们指定自己的python
  export PYSPARK_PYTHON=/opt/python3/bin/python
  ```

- 把整个 spark 文件夹复制到其他节点上去

  ```sh
  scp -r spark-1.6.0 hadoop2:/opt
  ```

- 进入 spark 的 sbin 目录，执行当前目录下的 `start-all.sh`

  ```python
  ./start-all.sh
  ```

- 这样就启动成功了

Web UI  `hadoop1:8080`

Spark `spark://hadoop:7077`

如果要修改默认端口，只需要修改  sbin/start-master.sh 即可

```sh
66 行左右

if [ "$SPARK_MASTER_PORT" = "" ]; then
  SPARK_MASTER_PORT=7077
fi

if [ "$SPARK_MASTER_WEBUI_PORT" = "" ]; then
  SPARK_MASTER_WEBUI_PORT=8080
```

提交任务方式

- 启动 Spark  `./spark.all.sh`

`./spark-submit.sh --master spark://hadoop1:7077 --deploy-mode

> 基于 Yarn 的集群模式

这个模式的话，就是配置文件需要改

修改成这样就可以

```sh
export SPARK_MASTER_IP=hadoop1
export SPARK_MASTER_PORT=7077
export SPARK_WORKER_CORES=2
export JAVA_HOME=/opt/jdk1.8.0_171

export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop

```

然后启动的时候，需要启动 yarn hdfs 不需要提前启动 spark ,

直接利用 spark-submit 提交任务就行






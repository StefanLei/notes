#### 补充部分算子

##### transformation 

> join, leftOuterJoin, rightOuterJoin, fullOuterJoin
>
> 作用在键值对格式的RDD上，根据Key进行连接，对(K,V) join (K,W) 返回 （K,(V,W))

```python
from pyspark import SparkConf, SparkContext

conf = SparkConf().setAppName('wordcount').setMaster('local')
sc = SparkContext(conf=conf)
data1 = [("a", 3), ("b", 4), ("a", 3), ("b", 6)]
data2 = [("a", 2), ("a", 3), ('c', 5)]

rdd1 = sc.parallelize(data1)
rdd2 = sc.parallelize(data2)

# 内连接，取两个 rdd 共同的 key。value的顺序以rdd的顺序为准。
# result = rdd2.join(rdd1)
# print(result.collect())
[('a', (2, 3)), ('a', (2, 3)), ('a', (3, 3)), ('a', (3, 3))]

# 左外连接，以左边的为准。value的顺序以rdd的顺序为准。
# result = rdd1.leftOuterJoin(rdd2)
# print(result.collect())
[('b', (4, None)), ('b', (6, None)), ('a', (3, 2)), ('a', (3, 3)), ('a', (3, 2)), ('a', (3, 3))]

# 右外连接，以右边的为准。value的顺序以rdd的顺序为准。
# result = rdd1.rightOuterJoin(rdd2)
# print(result.collect())
[('c', (None, 5)), ('a', (3, 2)), ('a', (3, 3)), ('a', (3, 2)), ('a', (3, 3))]

```

> union 并集合

```python
from pyspark import SparkConf, SparkContext

conf = SparkConf().setAppName('wordcount').setMaster('local')
sc = SparkContext(conf=conf)
data1 = [("a", 3), ("b", 4), ("a", 3), ("b", 6)]
data2 = [("a", 2), ("a", 3), ('c', 5)]

rdd1 = sc.parallelize(data1)
rdd2 = sc.parallelize(data2)

# 把两个rdd数据拼起来，类似于并集，但是不去重
result = rdd1.union(rdd2)
print(result.collect())
```

> intersection 交集

```python
from pyspark import SparkConf, SparkContext

conf = SparkConf().setAppName('wordcount').setMaster('local')
sc = SparkContext(conf=conf)
data1 = [("a", 3), ("b", 4), ("a", 3), ("b", 6)]
data2 = [("a", 2), ("a", 3), ('c', 5)]

rdd1 = sc.parallelize(data1)
rdd2 = sc.parallelize(data2)

# 交集,返回共有的元素
result = rdd1.intersection(rdd2)
print(result.collect())
```

> subtract 差集

```python
from pyspark import SparkConf, SparkContext

conf = SparkConf().setAppName('wordcount').setMaster('local')
sc = SparkContext(conf=conf)
data1 = [("a", 3), ("b", 4), ("a", 3), ("b", 6)]
data2 = [("a", 2), ("a", 3), ('c', 5)]

rdd1 = sc.parallelize(data1)
rdd2 = sc.parallelize(data2)

# 差集 rdd1-rdd2
result = rdd1.subtract(rdd2)
print(result.collect())
# rdd2-rdd1
result = rdd2.subtract(rdd1)
print(result.collect())

```

> mapPartions() 这个是用函数处理，一个分区。是否处理每一个元素，由我们自己函数决定
>
> map() 是处理分区里面的每一个元素。这个默认内部调用的就是 python map函数

```python
from pyspark import SparkConf, SparkContext
conf = SparkConf().setAppName('wordcount').setMaster('local')
sc = SparkContext(conf=conf)

rdd = sc.parallelize([1, 2, 3, 4, 5, 6])

# 这个x是整个分区的元素的，迭代器。这个之所以用yield的元素是，mapPartitions 会把test函数的结果，list()处理，如果返回值是整数，如果还list就会报错，所以需要加yield。但是如果返回值是列表，那么可以用return
def test(x):
    yield sum(x)

    
def test1(x):
    return x

print(rdd.mapPartitions(test).collect())
print(rdd.mapPartitions(lambda x: x))
-----------------------------------------------------------------
# 也可以这样遍历处理
def wordcount(x):
    for item in x:
        yield (item, 1)

        
print(rdd.mapPartitions(wordcount).collect())
```

```python
RDD.map(func) 相当于内部调用了，python的map函数，以及func函数
RDD.mapPartitions(func) 相当于内部调用了func函数，如果想要实现map的功能，可以自己遍历每一个元素实现
```

> distinct() 去重。对某一个 RDD 去重

```python
from pyspark import SparkConf, SparkContext
import os

# os.environ["PYSPARK_PYTHON"] = "/opt/Python3/bin/python3.6"

sc = SparkContext(master="local", appName='wordcount')
data1 = [("a", 3), ("b", 4), ("a", 3), ("b", 6)]
data2 = [("a", 2), ("a", 3), ('c', 5)]

rdd1 = sc.parallelize(data1)
rdd2 = sc.parallelize(data2)

# 对RDD去重
print(rdd1.distinct().collect())

[('a', 3), ('b', 4), ('b', 6)]
```

> filter 过滤。和Python中的filter一样，满足条件的保留

```python
from pyspark import SparkConf, SparkContext


sc = SparkContext(master="local", appName='wordcount')
data1 = [("a", 3), ("b", 4), ("a", 3), ("b", 6)]
data2 = [("a", 2), ("a", 3), ('c', 5)]

rdd1 = sc.parallelize(data1)
rdd2 = sc.parallelize(data2)

# 这里就是当value大于3的保留
print(rdd1.filter(lambda x: x[1] >= 3).collect())

```

> foreach(func) foreachPartition() 遍历

```python
from pyspark import SparkConf, SparkContext

sc = SparkContext(master="local", appName='wordcount')
data1 = [("a", 3), ("b", 4), ("a", 3), ("b", 6)]
data2 = [("a", 2), ("a", 3), ('c', 5)]

rdd1 = sc.parallelize(data1)
rdd2 = sc.parallelize(data2)


# foreach x是每一个元素
rdd1.foreach(lambda x: print(x))

def test(x):
    for item in x:
        print(item)

# foreachPartition x是每一个分区，这个是高性能的算子
rdd1.foreachPartition(test)
```


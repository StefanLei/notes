### 缓存机制

==默认是不缓存的==

我们可以使用 `cache()` 把数据集缓存到内存中去，然后第二次读取相同数据集的时候，就会从内存中读取。

`cache()` 也是惰性的，意味着第一次调用了 `action`算子后，才会对 RDD 进行缓存，第二次对这个 RDD 使用 Action 算子的时候，就会从内存读取

==从 Driver 端的输出日志中可以看到缓存==

```python
from pyspark import SparkConf, SparkContext, StorageLevel
import time

conf = SparkConf().setAppName('wordcount').setMaster('local')
sc = SparkContext(conf=conf)

myfile = sc.textFile(r'd:\aipai.sql')
# 这里设置了缓存后，第二次对这个 RDD 使用 Action 算子的时候，就会从内存读取
myfile.cache()

start = time.clock()
# 这是第一次
myfile.count()
end = time.clock()
print(end - start)

start = time.clock()
# 这里就是第二次，这里会从内存加载数据
myfile.count()
end = time.clock()
print(end - start)

```



#### 持久化级别

调用 cache() 将会在 executor 的内存中持久化保存 RDD 的每个分区，如果 exector 没有足够的内存来储存RDD 分区，计算并不会失败，只不过是根据需要 重新计算分区，而不使用缓存

对于包含大量转换操作的复杂程序来说，重新技术的代价高，因此 spark 提供了不同级别的持久化行为，我们可以通过调用 persist() 来指定 StorageLevel 参数来做出选择

`cache()`默认的持久化级别是 `MEMORY_ONLY`，它使用对象在内存中的常规表示方法。另一种更紧凑的表示方法是通过把分区中的元素序列化为字节数组来实现。这一级别称为`MEMORY_ONLY_SER`。

与 `MEMORY_ONLY`相比，`MEMORY_ONLY_SER`多了一份cpu开销，但是如果它生成的序列化 RDD 分区大小(字节数组比对象小)适合保存到内存中，而常规的表示方法无法做到这一时，这份额外的开销的值得的。

`MEMORY_ONLY_SER`还能减少垃圾回收的压力，因为每个RDD被存为一个字节数组，而不是大量的对象。



如果重新计算数据集的代价太过高昂，那么可以使用 `MEMORY_AND_DISK`(如果数据集的大小不适合保存到内存中，就将其溢写到磁盘)。或者 `MEMORY_AND_DISK_SER`（如果序列化数据集的大小不适合保存在内存，就将其溢写到磁盘）
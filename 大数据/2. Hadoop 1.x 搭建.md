#### Hadoop 1.x HDFS 搭建和配置

我这里有 3 台机器

- node1   192.168.1.101   作为 `NameNode` 同时作为 `DataNode` 当然不推荐既是 NameNode 又是 DataNode
- node2   192.168.1.102    作为 `Second NameNode`  同时作为 `DataNode `
- node3   192.168.1.103    作为 `DataNode`

==因为每个节点的配置是一样的，所以我们就配置一个，然后最后复制给其他节点==

##### 下载 Hadoop 2.x ，虽然我们配置的是 Hadoop 1.x 的 HDFS ，但是也可以，2 可以兼容 1 的搭建方式

- 安装 Java 环境  （同时在系统里面配置环境变量）
- 时间同步 `ntpdate`
- SSH 免密码登录设置

##### 下载解压 Hadoop 2.x 我们这里放到 /opt 目录下

> tar -zxvf Hadoop2.5.6

##### 配置 hadoop-env.sh

> vim /opt/hadoop2.5.6/etc/hadoop/hadoop-env.sh

这里面我们主要配置 Java 的环境变量

```bash
export JAVA_HOME=/opt/java
export HADOOP_PREFIX=/usr/local/hadoop   //可不配 
```

##### 配置 core-site.xml 

> 这个文件主要用来配置，哪个节点（服务器）是 `NameNode`

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
       	// 这里配置的是 NameNode 节点对应的地址，用域名和IP都可以，如果用域名，需要到每个机器上设置
        <value>hdfs://hadoop1:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        // 这里配置的是默认的 NameNode 和 DataNode 的存放位置
        <value>/opt/hadoop-tmp-dir</value>
    </property>
</configuration>
```

##### 配置 hdfs-site.xml

> 这里配置的是，哪个节点作为 （服务器）是 `SecondNode` 节点

可以和 NameNode 节点相同，但是不建议

```xml
<configuration>
    <property>
        // 这里配置的是，每个 block 块最多有多少副本，这个和 DataNode 个数没关系
        <name>dfs.replication</name>
        <value>4</value>
    </property>
    <property>
        // 如果设置了这个，就可以不需要 master 文件
        <name>dfs.namenode.secondary.http-address</name>
        <value>hadoop2:50090</value>
    </property>
    <property>
        // 如果设置了这个，就可以不需要 master 文件
        <name>dfs.namenode.secondary.https-address</name>
        <value>hadoop2:50091</value>
    </property>
</configuration>
```

##### Masters: 文件 （设置 second NameNode） 

在/home/hadoop-2.6.5/etc/hadoop/新建 masters 文件 写上  second NameNode节点名 hadoop2

> 如果在 hdfs-site.xml 设置了，<name>dfs.namenode.secondary.http-address</name>
>
> masters 就可以不需要

##### Slaves 设置 DateNode节点，NameNode 可以同时是 DateNode 节点

在/home/hadoop-2.5.1/etc/hadoop/slaves 文件中填写 DateNode 节点名，一行写一个

```python
hadoop1
hadoop2
hadoop3
hadoop4
```

##### 配置 Hadoop 的环境变量

```sh
export HADOOP_HOME/home/hadoop-2.6.5 
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

#### 最后所有的配置文件 ，发送给其他的服务器



##### 格式化 HDFS 

对 HDFS 进行格式化  `hdfs namenode -format `

##### 使用

启动 HDFS： start-dfs.sh 

关闭 HDFS：  stop-dfs.sh

##### 关闭防火墙

`service iptables stop `

`systemctl stop firewalld`

##### 浏览器访问 默认是 NameNode 的路径 

在浏览器输入 hadoop1:50070 出现以下界面成功 